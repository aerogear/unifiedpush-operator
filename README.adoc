:toc:
:toc-placement!:

// gEmoji for admonitions, see
// https://gist.github.com/dcode/0cfbf2699a1fe9b46ff04c41721dda74#admonitions
ifdef::env-github[]
:tip-caption: :bulb:
:note-caption: :information_source:
:important-caption: :heavy_exclamation_mark:
:caution-caption: :fire:
:warning-caption: :warning:
endif::[]

// Links (alphabetical order)
:apache_license: http://www.apache.org/licenses/LICENSE-2.0[Apache License, Version 2.0]
:application_monitoring_operator: https://github.com/integr8ly/application-monitoring-operator[application-monitoring-operator]
:code_of_conduct: link:CODE_OF_CONDUCT.md[Contributor Code of Conduct]
:export_policy: https://aerogear.org/legal/export.html[AeroGear Export Policy]
:aerogear_freenode: irc://irc.freenode.net/aerogear[#aerogear on FreeNode IRC]
:aerogear_jira: https://issues.jboss.org/projects/AEROGEAR/issues[AeroGear on JBoss Jira]
:aerogear_matrix: https://matrix.to/#/!IipcvbGVqkiTUQauSC:matrix.org[#aerogear:matrix.org on Matrix]
:mailing_list: https://groups.google.com/forum/#!forum/aerogear[Google Groups Mailing List]
:minishift: https://github.com/minishift/minishift[Minishift]
:rh_product_security: https://access.redhat.com/security/team/contact[Red Hat Product Security team]

= UnifiedPush Operator

The UnifiedPush Operator for Kubernetes provides an easy way to
install and manage an AeroGear UnifiedPush Server on Kubernetes.

toc::[]

== Limitations

// https://issues.jboss.org/browse/AEROGEAR-9162
[NOTE]
====
This operator currently only works on OpenShift. This is because it
provisions `Route`s and `ImageStream`s, and also relies on the fact that OpenShift
provides an OAuth server. In future we aim to make it work on vanilla
Kubernetes also.
====

== Installation

As a user with admin permissions, you can easily install the
unifiedpush-operator in your OpenShift cluster as follows:

....
kubectl create namespace unifiedpush
kubectl create -n unifiedpush -f deploy/service_account.yaml
kubectl create -n unifiedpush -f deploy/role.yaml
kubectl create -n unifiedpush -f deploy/role_binding.yaml
kubectl create -n unifiedpush -f deploy/crds/push_v1alpha1_unifiedpushserver_crd.yaml
kubectl create -n unifiedpush -f deploy/crds/push_v1alpha1_pushapplication_crd.yaml
kubectl create -n unifiedpush -f deploy/crds/push_v1alpha1_androidvariant_crd.yaml
kubectl create -n unifiedpush -f deploy/crds/push_v1alpha1_iosvariant_crd.yaml
kubectl create -n unifiedpush -f deploy/operator.yaml
....

== Configuration

=== Image Streams

The operator uses 3 image streams and what image streams to use are configurable
with environment variables.

Unified Push Server and Oauth proxy image stream are created within the same namespace by the operator.
However, for Postgres the image stream in `openshift` namespace is used.

The following table shows the available
environment variable names, along with their default values:

.Environment Variables
|===
|Name |Default |Purpose

|`UPS_IMAGE_STREAM_NAME`
|`ups-imagestream`
| Name of the Unified Push image stream that will be created by the operator.

|`UPS_IMAGE_STREAM_TAG`
|`latest`
| Tag of the Unified Push image stream that will be created by the operator.

|`UPS_IMAGE_STREAM_INITIAL_IMAGE`
|`docker.io/aerogear/unifiedpush-wildfly-plain:2.2.1.Final`
| Initial image for the Unified Push image stream that will be created by the operator.

|`OAUTH_PROXY_IMAGE_STREAM_NAME`
|`ups-oauth-proxy-imagestream`
| Name of the Oauth proxy image stream that will be created by the operator.

|`OAUTH_PROXY_IMAGE_STREAM_TAG`
|`latest`
| Tag of the Oauth proxy image stream that will be created by the operator.

|`OAUTH_PROXY_IMAGE_STREAM_INITIAL_IMAGE`
|`docker.io/openshift/oauth-proxy:v1.1.0`
| Initial image for the Oauth proxy image stream that will be created by the operator.

|`POSTGRES_IMAGE_STREAM_NAMESPACE`
|`openshift`
| Namespace to look for the Postgres image stream.

|`POSTGRES_IMAGE_STREAM_NAME`
|`postgresql`
| Name of the Postgres image stream to look for.

|`OAUTH_PROXY_IMAGE_STREAM_TAG`
|`10`
| Tag of the Postgres image stream.

|===

CAUTION: Re-deploying this operator with customized images will cause
_all_ instances owned by the operator to be updated.


=== Container Names

If you would like to modify the container names, you can use the following environment variables.

.Environment Variables
|===
|Name |Default

|`UPS_CONTAINER_NAME`
|`ups`

|`OAUTH_PROXY_CONTAINER_NAME`
|`ups-oauth-proxy`

|`POSTGRES_CONTAINER_NAME`
|`postgresql`

|===


=== Backups

The `BACKUP_IMAGE` environment variable configures what image to use for backing up
the custom resources created by this operator. Default value is `quay.io/integreatly/backup-container:1.0.8`.


=== Metrics

The application-monitoring stack provisioned by the
{application_monitoring_operator} can be used to gather metrics from
the operator here.  Once you have provisioned that (or the
ServiceMonitor CRD at a minimum), you can run the following commands
to configure it:

....
kubectl label namespace unifiedpush monitoring-key=middleware
kubectl create -n unifiedpush -f deploy/service_monitor.yaml
....

== Custom Resources (aka How to get value from this operator)

=== UnifiedPushServer

This is the main installation resource kind. Creation of a valid
UnifiedPushServer CR will result in a functional AeroGear
UnifiedPushServer deployed to your namespace.

[NOTE]
====
This operator currently only supports one UnifiedPushServer CR to be
created.
====

Here are all of the configurable fields in a UnifiedPushServer:

.UnifiedPushServer fields
|===
|Field Name |Description

|backups
|A list of backup entries that CronJobs will be created from. See
 `./deploy/crds/push_v1alpha1_unifiedpushserver_cr_with_backup.yaml`
 for an annotated example. Note that a ServiceAccount called
 "backupjob" must already exist before the operator will create any
 backup CronJobs. See
 https://github.com/integr8ly/backup-container-image/tree/master/templates/openshift/rbac
 for an example.
|===

The most basic UnifiedPushServer CR doesn't specify anything in the
Spec section, so the example in
`./deploy/crds/push_v1alpha1_unifiedpushserver_cr.yaml` is a good
template:

.push_v1alpha1_unifiedpushserver_cr.yaml
[source,yaml]
----
apiVersion: push.aerogear.org/v1alpha1
kind: UnifiedPushServer
metadata:
  name: example-unifiedpushserver
----

To create this, you can run:

....
kubectl apply -n unifiedpush -f ./deploy/crds/push_v1alpha1_unifiedpushserver_cr.yaml
....

To see the created instance then, you can run:

....
kubectl get ups example-unifiedpushserver -n unifiedpush -o yaml
....

=== PushApplication

Once you've got your `UnifiedPushServer` up and running, you can
create a `PushApplication`.

The only configurable fields in a `PushApplication` are the name and
description, like the example in
`./deploy/crds/push_v1alpha1_pushapplication_cr.yaml`:

.push_v1alpha1_pushapplication_cr.yaml
[source,yaml]
----
apiVersion: push.aerogear.org/v1alpha1
kind: PushApplication
metadata:
  name: example-pushapplication
spec:
  description: 'An example push application to demonstrate the
    unifiedpush-operator'
----

To create this, you can run:

....
kubectl apply -n unifiedpush -f ./deploy/crds/push_v1alpha1_pushapplication_cr.yaml
....

To see the created instance then, you can run:

....
kubectl get pushApplication example-pushapplication -n unifiedpush -o yaml
....

Shortly after it's created, you should be able to see it in the list
of Applications in the UnifiedPush Admin UI, and you should also be
able to see the `pushApplicationId` and `masterSecret` fields on the
`status` object of your PushApplication instance in Kubernetes.

=== AndroidVariant

After creating the PushApplication above, you should be able to get
the `pushApplicationId` from the status, this will be needed to be
able to create Variants:

....
kubectl get pushApplication example-pushapplication -n unifiedpush -o jsonpath='{.status.pushApplicationId}'
....

Here are all of the configurable fields in an AndroidVariant:

.AndroidVariant fields
|===
|Field Name |Description

|pushApplicationId
|ID of the PushApplication that this variant corresponds to

|description
|Human friendly description for the variant

|senderId
|The "Google Project Number from the API Console

|serverKey
|The key from the Firebase Console of a project which has been enabled for FCM
|===

There's an example at
`./deploy/crds/push_v1alpha1_androidvariant_cr.yaml` that can be
modified and created as follows:

....
kubectl apply -n unifiedpush -f ./deploy/crds/push_v1alpha1_androidvariant_cr.yaml
....

=== IOSVariant

After creating the PushApplication above, you should be able to get
the `pushApplicationId` from the status, this will be needed to be
able to create Variants:

....
kubectl get PushApplication example-pushapplication -n unifiedpush -o jsonpath='{.status.pushApplicationId}'
....

Here are all of the configurable fields in an IOSVariant:

.IOSVariant fields
|===
|Field Name |Description

|pushApplicationId
|ID of the PushApplication that this variant corresponds to

|description
|Human friendly description for the variant

|certificate
|The base64 encoded APNs certificate that is needed to establish a
 connection to Apple's APNs Push Servers

|passphrase
|The APNs passphrase that is needed to establish a connection to
 Apple's APNs Push Servers

|production
|If `true`, indicates that a connection to production APNs server should
 be used. If `false` a connection to the Sandbox/Development APNs server
 will be used.
|===

There's an example at
`./deploy/crds/push_v1alpha1_iosvariant_cr.yaml` that can be
modified and created as follows:

....
kubectl apply -n unifiedpush -f ./deploy/crds/push_v1alpha1_iosvariant_cr.yaml
....

== Getting help

All AeroGear projects use the same communication channels.

*Issue tracker*

Our main issue tracker is {aerogear_jira}. Issues may also be created
here on GitHub for individual projects.

*Chat*

For synchronous real-time chat, we use Matrix/IRC. These are bridged
together, so you can choose which is more convenient for you:
{aerogear_matrix} or {aerogear_freenode}.

*Discussion list*

For important conversations, we discuss asynchronously on this
{mailing_list}. This is great for discussions that should involve many
people in different time zones, and allows us to easily link back to
conversations in future.

== Development

=== Prerequisites

- Access to an OpenShift cluster with admin privileges to be able to
  create Roles.  {minishift} is suggested.

- Go, Make, dep, operator-sdk, kubectl (kubectl can just be a symlink
  to oc)

=== Running the operator

1. Prepare the operator project:

....
make cluster/prepare
....

2. Run the operator (locally, not in OpenShift):

....
make code/run
....

3. Create a UPS instance (in another terminal):

....
kubectl apply -f deploy/crds/push_v1alpha1_unifiedpushserver_cr.yaml -n unifiedpush
....

4. Watch the status of your UPS instance provisioning (optional):

....
watch -n1 "kubectl get po -n unifiedpush && echo '' && kubectl get ups -o yaml -n unifiedpush"
....

5. If you want to be able to work with resources that require the
local instance of your operator to be able to talk to the UPS instance
in the cluster, then you'll need to make a corresponding domain name
available locally. Something like the following should work, by adding
an entry to /etc/hosts for the example Service that's created, then
forwarding the port from the relevant Pod in the cluster to the local
machine. Run this in a separate terminal, and ctrl+c to clean it up
when finished:

// TODO: We could maybe use a non-privileged port instead of :80?
....
# su/sudo is needed to be able to:
# - modify /etc/hosts
# - bind to port :80
KUBECONFIG=$HOME/.kube/config su -c "echo '127.0.0.1   example-unifiedpushserver-unifiedpush' >> /etc/hosts && kubectl port-forward $(kubectl get po -l service=ups -o name) 80:8080 && sed -i -e 's/^127.0.0.1   example-unifiedpushserver-unifiedpush$//g' -e '/^[[:space:]]*$/d' /etc/hosts"
....

6. When finished, clean up:

....
make cluster/clean
....

== Testing

=== Run unit tests

....
make test/unit
....

=== Run e2e tests

. Export env vars used in commands below

....
export NAMESPACE="<name-of-your-openshift-project-used-for-testing>"
export IMAGE="quay.io/<your-account-name>/unifiedpush-operator"
....

. Login to OpenShift cluster as a user with cluster-admin role

....
oc login <url> --token <token>
....

. Prepare a new OpenShift project for testing

....
make NAMESPACE=$NAMESPACE cluster/prepare
....

. Modify the operator image name in manifest file

....
yq w -i deploy/operator.yaml spec.template.spec.containers[0].image $IMAGE
....

Note: If you do not have link:https://mikefarah.github.io/yq/[yq] installed, just simply edit the image name in link:deploy/operator.yaml[deploy/operator.yaml]

. Build & push the operator container image to your Dockerhub/Quay image repository, e.g.

....
operator-sdk build $IMAGE --enable-tests && docker push $IMAGE
....

. Run the test

....
operator-sdk test cluster $IMAGE --namespace $NAMESPACE --service-account unifiedpush-operator
....

== Security Response

If you've found a security issue that you'd like to disclose
confidentially please contact the {rh_product_security}.

== Legal

The UnifiedPush Operator is licensed under the {apache_license}
License, and is subject to the {export_policy}.
